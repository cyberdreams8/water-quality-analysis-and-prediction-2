{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyberdreams8/water-quality-analysis-and-prediction-2/blob/main/PDS_waterquality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing the necessary liabraries"
      ],
      "metadata": {
        "id": "Kcd40yZsV9rh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G162dnyHI6gp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reading the Dataset"
      ],
      "metadata": {
        "id": "_NWJq9V-WFfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = pd.read_csv(\"https://raw.githubusercontent.com/cyberdreams8/water-quality-analysis-and-prediction-2/refs/heads/main/water_quality_dataset.csv\")\n",
        "Dataset"
      ],
      "metadata": {
        "id": "uSSjr7ikJB4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.columns"
      ],
      "metadata": {
        "id": "syTepilOrgba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "4kkmdiyft_22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.head()"
      ],
      "metadata": {
        "id": "P5N2mbUHWjuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.tail()"
      ],
      "metadata": {
        "id": "L_ppXMi5WnE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sanity Check of Data"
      ],
      "metadata": {
        "id": "lPC0JsE9Wsl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.shape"
      ],
      "metadata": {
        "id": "JjwkHgADWqK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.info()"
      ],
      "metadata": {
        "id": "PDpJ16KFW0AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding missing values\n",
        "Dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "CpCCrD30W2wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding missing values in percentage\n",
        "Dataset.isnull().sum()/Dataset.shape[0]*100"
      ],
      "metadata": {
        "id": "TsluLFNVW9hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding duplicates\n",
        "Dataset.duplicated().sum()"
      ],
      "metadata": {
        "id": "8AqXY-gzXYFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#identifying garbage values\n",
        "for i in Dataset.select_dtypes(include='object').columns:\n",
        "  print(Dataset[i].value_counts())\n",
        "  print(\"***\"*10)"
      ],
      "metadata": {
        "id": "9jCwki_AXgX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "VV5aDL8UY4eh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.columns"
      ],
      "metadata": {
        "id": "mJc5ThL4ZVTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#descriptive statistics\n",
        "columns= ['Temperature Min', 'Temperature Max', 'pH Min', 'pH Max',\n",
        "       'Conductivity (µmhos/cm) Min', 'Conductivity (µmhos/cm) Max',\n",
        "       'Turbidity (NTU)', 'Dissolved Oxygen (mg/L)', 'BOD (mg/L)',\n",
        "       'TDS (mg/L)', 'Hardness (mg/L)']\n",
        "selected_data = Dataset[columns]\n",
        "selected_data.describe().T"
      ],
      "metadata": {
        "id": "SukRavqvY07X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_data2 = Dataset[['STATE','Potability']]\n",
        "selected_data2.describe().T"
      ],
      "metadata": {
        "id": "sy6Rh3JDbg7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: upsample minority of potability\n",
        "\n",
        "# Assuming 'Dataset' DataFrame from the previous code is available.\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = Dataset.drop('Potability', axis=1)\n",
        "y = Dataset['Potability']\n",
        "\n",
        "# Initialize RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=42)  # You can change the random state\n",
        "\n",
        "# Resample the dataset\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "# Create a new DataFrame with the resampled data\n",
        "Dataset = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "Dataset['Potability'] = y_resampled\n",
        "\n",
        "# Print the value counts of the 'Potability' column to verify the upsampling\n",
        "print(Dataset['Potability'].value_counts())\n",
        "\n",
        "# Now 'upsampled_dataset' contains the upsampled data\n",
        "# You can further use this dataset for model training etc."
      ],
      "metadata": {
        "id": "3kG5xXpmZZ2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "V_a_74LYcXbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#histograms to understand the distribution\n",
        "for i in selected_data.columns:\n",
        "  sns.histplot(data=Dataset,x=i)\n",
        "  plt.title(i)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "XUbe1L09a4Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Boxplot to identify Outliers\n",
        "for i in selected_data.columns:\n",
        "  sns.boxplot(data=Dataset,x=i)\n",
        "  plt.title(i)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "SG9H0rJ1LLCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: give a best plotting technique which compares each attribute to potability other than histplot and boxplot\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'Dataset' and 'selected_data' are defined as in the previous code.\n",
        "\n",
        "# Create a list of attributes to compare with potability\n",
        "attributes = ['pH Min', 'pH Max', 'Turbidity (NTU)', 'Conductivity (µmhos/cm) Min',\n",
        "              'Conductivity (µmhos/cm) Max', 'Hardness (mg/L)', 'TDS (mg/L)']\n",
        "\n",
        "# Plotting using violinplots\n",
        "for attribute in attributes:\n",
        "    plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
        "    sns.violinplot(x='Potability', y=attribute, data=Dataset)\n",
        "    plt.title(f'{attribute} vs. Potability')\n",
        "    plt.xlabel('Potability')\n",
        "    plt.ylabel(attribute)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "CtFFlTjDOCRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correlation with heatmap to interpret the relation and multicolliniarity\n",
        "corr =selected_data.corr()\n",
        "corr"
      ],
      "metadata": {
        "id": "D3t_bWNNO0be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(13,13))\n",
        "sns.heatmap(corr, annot=True, cmap=\"YlGnBu\") # Change 'YlGnBu' to your desired colormap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A_UtxT5xPdO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MISSING VALUE TREATMENT"
      ],
      "metadata": {
        "id": "A21Hi73bQel9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "vVRw2PFCQii6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_impute = ['Turbidity (NTU)', 'Dissolved Oxygen (mg/L)', 'BOD (mg/L)', 'TDS (mg/L)', 'Hardness (mg/L)']\n",
        "\n",
        "# Apply median imputation for each column\n",
        "for column in columns_to_impute:\n",
        "    median_value = Dataset[column].median()\n",
        "    Dataset[column]=Dataset[column].fillna(median_value)"
      ],
      "metadata": {
        "id": "vYu_EEJEXS_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "ZH83zKL3Xrm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "knn_impute = ['Temperature Min', 'Temperature Max', 'pH Min', 'pH Max',\n",
        "                     'Conductivity (µmhos/cm) Min', 'Conductivity (µmhos/cm) Max']\n",
        "\n",
        "# Create a KNNImputer instance (using k=5 as a common choice)\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "# Apply KNN imputation only to the specified columns\n",
        "Dataset[knn_impute] = imputer.fit_transform(Dataset[knn_impute])\n"
      ],
      "metadata": {
        "id": "SRrfdPZNXx87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "-ZQAjb0CYV9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = Dataset.dropna(subset=['Station Code'])\n",
        "Dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "tiPVI4oDZH-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "HLBXKQx0Z3ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Handling Outliers"
      ],
      "metadata": {
        "id": "udC7niI2bJ23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in selected_data.columns:\n",
        "  sns.boxplot(data=Dataset,x=i)\n",
        "  plt.title(i)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "MxDJehM_aL4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wisker(col):\n",
        "  q1,q3 = np.percentile(col,[25,75])\n",
        "  iqr = q3-q1\n",
        "  lower_bound = q1 - (1.5*iqr)\n",
        "  upper_bound = q3 + (1.5*iqr)\n",
        "  return lower_bound,upper_bound"
      ],
      "metadata": {
        "id": "EQH0xBLpzk0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.columns"
      ],
      "metadata": {
        "id": "yajmjCgB6rbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for i in ['Temperature Min', 'Temperature Max', 'pH Min', 'pH Max',\n",
        "          'Conductivity (µmhos/cm) Min', 'Conductivity (µmhos/cm) Max']:\n",
        "    lower_bound, upper_bound = wisker(Dataset[i])\n",
        "\n",
        "    # Use .loc to limit values to within the whisker range\n",
        "    Dataset.loc[Dataset[i] > upper_bound, i] = upper_bound\n",
        "    Dataset.loc[Dataset[i] < lower_bound, i] = lower_bound\n"
      ],
      "metadata": {
        "id": "D75vZQpj5q7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in selected_data.columns:\n",
        "  sns.boxplot(data=Dataset,x=i)\n",
        "  plt.title(i)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "9SW5TjCG8jyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Added columns like Temperature Mean,pH Mean and Conductivity Mean"
      ],
      "metadata": {
        "id": "ArAsyJSVRc_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = Dataset.assign(\n",
        "       pH_Mean=(Dataset['pH Min'] + Dataset['pH Max']) / 2,\n",
        "       Conductivity_Mean=(Dataset['Conductivity (µmhos/cm) Min'] + Dataset['Conductivity (µmhos/cm) Max']) / 2,\n",
        "       Temperature_Mean=(Dataset['Temperature Min'] + Dataset['Temperature Max']) / 2\n",
        "   )\n"
      ],
      "metadata": {
        "id": "ZdF9qnQDO_9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "cWQZtoCcRUQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X8hzXksoNDjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA VISUALIZATION\n"
      ],
      "metadata": {
        "id": "S3rvHAJWIY2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assuming your data is loaded into a DataFrame called 'df'\n",
        "# Replace this with your actual data loading code\n",
        "# df = pd.read_csv('your_dataset.csv')"
      ],
      "metadata": {
        "id": "pPdCmGqfNEqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 1. Yearly Trend of Water Potability\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    yearly_potability = Dataset.groupby(['Year', 'Potability']).size().unstack()\n",
        "    yearly_potability_pct = yearly_potability.div(yearly_potability.sum(axis=1), axis=0) * 100\n",
        "    yearly_potability_pct.plot(kind='bar', stacked=True)\n",
        "    plt.title('Yearly Trend of Water Potability (2012-2021)')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Percentage')\n",
        "    plt.legend(title='Potability')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "HnRhtmihIZlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Year-wise Quality Trend:**\n",
        "\n",
        "\n",
        "1.Reveals long-term trends in water quality\n",
        "2.Shows effectiveness of water management policies\n",
        "3.Identifies periods of quality improvement or deterioration\n",
        "4.Helps in future water quality predictions\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uDFx9nfwNCII"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J823w1DJjerI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 2. State-wise Water Quality Heatmap\n",
        "plt.figure(figsize=(15, 8)) # Removed extra indentation here\n",
        "state_params = Dataset.groupby('STATE')[['TDS (mg/L)', 'Hardness (mg/L)', 'Turbidity (NTU)', 'BOD (mg/L)']].mean()\n",
        "sns.heatmap(state_params, annot=True, fmt='.2f', cmap='YlOrRd')\n",
        "plt.title('State-wise Average Water Quality Parameters')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0vKKX_beUP-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***State-wise Quality Comparison:***\n",
        "\n",
        "\n",
        "1.Compares overall water quality across states\n",
        "2.Identifies states needing immediate intervention\n",
        "3.Shows regional patterns in water quality\n",
        "4.Useful for resource allocation"
      ],
      "metadata": {
        "id": "-LB-LlejleCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # 3. Temporal Changes in pH Levels\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    yearly_ph = Dataset.groupby('Year')[['pH Min', 'pH Max']].mean()\n",
        "    plt.plot(yearly_ph.index, yearly_ph['pH Min'], 'b-', label='Min pH')\n",
        "    plt.plot(yearly_ph.index, yearly_ph['pH Max'], 'r-', label='Max pH')\n",
        "    plt.fill_between(yearly_ph.index, yearly_ph['pH Min'], yearly_ph['pH Max'], alpha=0.2)\n",
        "    plt.title('Temporal Changes in pH Levels (2012-2021)')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('pH Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KHE25g60UWje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***pH Range Analysis:***\n",
        "\n",
        "\n",
        "1.Shows state-wise variations in pH levels\n",
        "2.Identifies states with concerning pH ranges\n",
        "3.Helps in targeted pH treatment planning\n",
        "4.Useful for ecological impact assessment"
      ],
      "metadata": {
        "id": "Q36J3V6hlvK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # 4. Box Plot of TDS Distribution by State\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    sns.boxplot(x='STATE', y='TDS (mg/L)', data=Dataset)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.title('TDS Distribution by State')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "VIaxXaiSUazL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "95gpduNykNvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # 5. Yearly Trend of Average Conductivity\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    yearly_conductivity = Dataset.groupby('Year')[['Conductivity (µmhos/cm) Min', 'Conductivity (µmhos/cm) Max']].mean()\n",
        "    plt.plot(yearly_conductivity.index, yearly_conductivity['Conductivity (µmhos/cm) Max'],\n",
        "             'ro-', label='Max Conductivity')\n",
        "    plt.plot(yearly_conductivity.index, yearly_conductivity['Conductivity (µmhos/cm) Min'],\n",
        "             'bo-', label='Min Conductivity')\n",
        "    plt.title('Yearly Trend of Average Conductivity (2012-2021)')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Conductivity (μmhos/cm)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WaLbWqllUeqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # 6. Dissolved Oxygen vs BOD Scatter Plot with Year Color Coding\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(Dataset['Dissolved Oxygen (mg/L)'], Dataset['BOD (mg/L)'],\n",
        "                         c=Dataset['Year'], cmap='viridis')\n",
        "    plt.colorbar(scatter, label='Year')\n",
        "    plt.xlabel('Dissolved Oxygen (mg/L)')\n",
        "    plt.ylabel('BOD (mg/L)')\n",
        "    plt.title('Dissolved Oxygen vs BOD Relationship Over Years')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "UYrzRYV8Uhaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "paxa7Jk-X0UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # 7. Temperature Range by State and Season\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    state_temp = Dataset.groupby('STATE')[['Temperature Min', 'Temperature Max']].mean()\n",
        "    x = np.arange(len(state_temp.index))\n",
        "    width = 0.35\n",
        "    plt.bar(x - width/2, state_temp['Temperature Min'], width, label='Min Temperature')\n",
        "    plt.bar(x + width/2, state_temp['Temperature Max'], width, label='Max Temperature')\n",
        "    plt.xlabel('State')\n",
        "    plt.ylabel('Temperature (°C)')\n",
        "    plt.title('Temperature Range by State')\n",
        "    plt.xticks(x, state_temp.index, rotation=45)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5wy47_22Uj83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # 8. Yearly Changes in Water Hardness\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.boxplot(x='Year', y='Hardness (mg/L)', data=Dataset)\n",
        "    plt.title('Yearly Changes in Water Hardness')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Hardness (mg/L)')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TKGhXxFPUnEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Correlation Matrix Over Time\n",
        "plt.figure(figsize=(12, 8))\n",
        "numeric_cols = ['Temperature Max', 'pH Max', 'Conductivity (µmhos/cm) Max',\n",
        "               'Turbidity (NTU)', 'Dissolved Oxygen (mg/L)', 'BOD (mg/L)', 'TDS (mg/L)', 'Hardness (mg/L)']\n",
        "# Replacing 'df' with 'Dataset' to access the DataFrame containing the data\n",
        "correlation = Dataset[numeric_cols].corr()\n",
        "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Matrix of Water Quality Parameters')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mu2BU1KWUnwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # 10. State-wise Potability Analysis\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    state_potability = Dataset.groupby('STATE')['Potability'].value_counts(normalize=True).unstack()\n",
        "    state_potability.plot(kind='bar', stacked=True)\n",
        "    plt.title('State-wise Water Potability Distribution')\n",
        "    plt.xlabel('State')\n",
        "    plt.ylabel('Percentage')\n",
        "    plt.legend(title='Potability')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KwDzYHkrUrID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ENCODING"
      ],
      "metadata": {
        "id": "ANaJjp05iIjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#One-hot encode the 'State' column"
      ],
      "metadata": {
        "id": "udMlUAaeSink"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset['STATE'] = Dataset['STATE'].str.replace('Uttrakhand', 'Uttarakhand', case=False)"
      ],
      "metadata": {
        "id": "wg-U5mwyYbom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset['STATE'] = Dataset['STATE'].str.title().str.strip()"
      ],
      "metadata": {
        "id": "c5uEbYeiawDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = pd.get_dummies(Dataset, columns=[\"STATE\"], drop_first=True)\n",
        "Dataset"
      ],
      "metadata": {
        "id": "Hj_wv4yYRpTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i7VtHPJbUN2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.columns"
      ],
      "metadata": {
        "id": "gzl-1NTmUnK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoding Potability"
      ],
      "metadata": {
        "id": "xMLmbGvgiPBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset['Potability'] = Dataset['Potability'].map({'Drinkable': 1, 'Not Drinkable': 0})\n",
        "Dataset"
      ],
      "metadata": {
        "id": "tjOViR5XUVIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Normalization"
      ],
      "metadata": {
        "id": "SZNab6Ywh21J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Select numerical columns for normalization, excluding identifiers or binary flags\n",
        "numerical_columns = [\n",
        "    'Temperature Min', 'Temperature Max', 'pH Min', 'pH Max',\n",
        "    'Conductivity (µmhos/cm) Min', 'Conductivity (µmhos/cm) Max',\n",
        "    'Turbidity (NTU)', 'Dissolved Oxygen (mg/L)', 'BOD (mg/L)', 'TDS (mg/L)', 'Hardness (mg/L)',\n",
        "    'pH_Mean', 'Conductivity_Mean', 'Temperature_Mean'\n",
        "]\n",
        "\n",
        "# Initialize MinMaxScaler and normalize\n",
        "scaler = MinMaxScaler()\n",
        "Dataset[numerical_columns] = scaler.fit_transform(Dataset[numerical_columns])\n",
        "\n",
        "# Confirm normalization by displaying the summary statistics\n",
        "Dataset[numerical_columns].describe()\n"
      ],
      "metadata": {
        "id": "qD4VLFWTeCEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "OMnpq-eN2RFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ML MODEL IMPLEMENTATION FOR POTABILITY"
      ],
      "metadata": {
        "id": "A6h6BfxQibMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)MODEL 1 - LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "__u4s4XF1EAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = Dataset.drop(columns=['Potability','Station Code', 'Station Name','Year'])\n",
        "y = Dataset['Potability']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "logistic_reg_model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "logistic_reg_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = logistic_reg_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "_RwWQgLm1DYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print accuracy percent\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(classification_rep)"
      ],
      "metadata": {
        "id": "a3asEjWx4C_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)MODEL 2 - DECISION TREES"
      ],
      "metadata": {
        "id": "qJqFl6at4fgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: IMplement decision trees on above split\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)  # You can adjust hyperparameters here\n",
        "\n",
        "# Train the model\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_dt = decision_tree_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "classification_rep_dt = classification_report(y_test, y_pred_dt)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-LpGSVpf41IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Decision Tree Accuracy: {accuracy_dt * 100:.2f}%\")\n",
        "print(classification_rep_dt)"
      ],
      "metadata": {
        "id": "ugKDpmV74xmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)MODEL 3 - RANDOM FOREST\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3WuUpWcB5T7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: APPLY RANDOM FOREST ON ABOVE SPLIT\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "random_forest_model = RandomForestClassifier(random_state=42)  # You can adjust hyperparameters here\n",
        "\n",
        "# Train the model\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = random_forest_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "classification_rep_rf = classification_report(y_test, y_pred_rf)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2rjGuPBE59IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")\n",
        "print(classification_rep_rf)"
      ],
      "metadata": {
        "id": "sq2MaKhQ6GSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4)MODEL 4 - Support Vector Machine (SVM)\n",
        "\n"
      ],
      "metadata": {
        "id": "N3G9bqoC7mxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: implement svm on above split\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize the SVM model\n",
        "svm_model = SVC(kernel='linear', random_state=42)  # You can change the kernel (e.g., 'rbf', 'poly')\n",
        "\n",
        "# Train the model\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "classification_rep_svm = classification_report(y_test, y_pred_svm)\n",
        "\n"
      ],
      "metadata": {
        "id": "N3RbVv_n6ikU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"SVM Accuracy: {accuracy_svm * 100:.2f}%\")\n",
        "print(classification_rep_svm)"
      ],
      "metadata": {
        "id": "GGKRSfK965QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5)MODEL 5 - K-Nearest Neighbors (KNN)"
      ],
      "metadata": {
        "id": "vPZ4EilO8jw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: implement knn\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined from previous code\n",
        "\n",
        "# Initialize the KNN classifier\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors\n",
        "\n",
        "# Train the model\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "classification_rep_knn = classification_report(y_test, y_pred_knn)\n",
        "\n"
      ],
      "metadata": {
        "id": "QKZ30O808btw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"KNN Accuracy: {accuracy_knn * 100:.2f}%\")\n",
        "print(classification_rep_knn)"
      ],
      "metadata": {
        "id": "1J7QbEMl8roo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6)MODEL 6 - NAIVE BAYES"
      ],
      "metadata": {
        "id": "1BCTmf569Rpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: implement naive bayes\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Initialize the Gaussian Naive Bayes model\n",
        "naive_bayes_model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "naive_bayes_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_nb = naive_bayes_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "classification_rep_nb = classification_report(y_test, y_pred_nb)\n",
        "\n"
      ],
      "metadata": {
        "id": "XmGGl7uf8kdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Naive Bayes Accuracy: {accuracy_nb * 100:.2f}%\")\n",
        "print(classification_rep_nb)"
      ],
      "metadata": {
        "id": "hZCK08yv8tj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: show a table with TWO COLUMNS MODEL USED AND THEIR ACCURACY\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Model Used': ['Logistic Regression', 'Decision Tree', 'Random Forest', 'Support Vector Machine (SVM)', 'K-Nearest Neighbors (KNN)', 'Naive Bayes'],\n",
        "    'Accuracy': [accuracy * 100, accuracy_dt * 100, accuracy_rf * 100, accuracy_svm * 100, accuracy_knn * 100, accuracy_nb * 100]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ],
      "metadata": {
        "id": "McZEyT0B9Nvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ML MODEL IMPLEMENTATION FOR CALCULATION OF WATER QUALITY INDEX (WQI)"
      ],
      "metadata": {
        "id": "zObarSGz3_XL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding a column of Water Quality Index (WQI) based on other parameters\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Define a function to calculate WQI\n",
        "def calculate_wqi(df):\n",
        "    weights = {\n",
        "        'pH_Mean': 0.15,\n",
        "        'Conductivity_Mean': 0.1,\n",
        "        'Temperature_Mean': 0.1,\n",
        "        'Turbidity (NTU)': 0.1,\n",
        "        'Dissolved Oxygen (mg/L)': 0.2,\n",
        "        'BOD (mg/L)': 0.15,\n",
        "        'TDS (mg/L)': 0.1,\n",
        "        'Hardness (mg/L)': 0.1\n",
        "    }\n",
        "\n",
        "    ideal_ranges = {\n",
        "        'pH_Mean': (6.5, 8.5),\n",
        "        'Conductivity_Mean': (0, 500),\n",
        "        'Temperature_Mean': (0, 35),\n",
        "        'Turbidity (NTU)': (0, 5),\n",
        "        'Dissolved Oxygen (mg/L)': (5, 14),\n",
        "        'BOD (mg/L)': (0, 3),\n",
        "        'TDS (mg/L)': (0, 500),\n",
        "        'Hardness (mg/L)': (0, 300)\n",
        "    }\n",
        "\n",
        "    def calculate_sub_index(value, param):\n",
        "        min_val, max_val = ideal_ranges[param]\n",
        "        if value < min_val:\n",
        "            return 100\n",
        "        elif value > max_val:\n",
        "            return 0\n",
        "        else:\n",
        "            return ((value - min_val) / (max_val - min_val)) * 100\n",
        "\n",
        "    # The following line was incorrectly indented, moved it to the correct level\n",
        "    wqi_list = []\n",
        "    for _, row in df.iterrows():\n",
        "        wqi_score = 0\n",
        "        for param, weight in weights.items():\n",
        "            if param in row:\n",
        "                sub_index = calculate_sub_index(row[param], param)\n",
        "                wqi_score += weight * sub_index\n",
        "        wqi_list.append(wqi_score)\n",
        "\n",
        "    df['WQI'] = wqi_list\n",
        "    return df\n",
        "\n",
        "# Step 1: Select relevant parameters\n",
        "params = ['pH_Mean', 'Conductivity_Mean', 'Temperature_Mean',\n",
        "          'Turbidity (NTU)', 'Dissolved Oxygen (mg/L)',\n",
        "          'BOD (mg/L)', 'TDS (mg/L)', 'Hardness (mg/L)']\n",
        "data_params = Dataset[params].dropna()\n",
        "\n",
        "# Step 2: Standardize the data\n",
        "scaler = StandardScaler() # Use the imported StandardScaler\n",
        "data_scaled = scaler.fit_transform(data_params)\n",
        "\n",
        "# Step 3: Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "Dataset['Cluster'] = kmeans.fit_predict(data_scaled) # Changed data to Dataset\n",
        "\n",
        "# Step 4: Calculate WQI\n",
        "data_with_wqi = calculate_wqi(Dataset) # Changed data to Dataset\n",
        "\n",
        "# Step 5: Calculate average WQI per cluster\n",
        "cluster_summary = data_with_wqi.groupby('Cluster')['WQI'].mean()\n",
        "\n",
        "# Display the results\n",
        "print(Dataset[['Station Code', 'Station Name', 'Cluster', 'WQI']].head())\n",
        "print(\"Average WQI for each cluster:\\n\", cluster_summary)"
      ],
      "metadata": {
        "id": "xYjji1Y__yFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "1oqsP40PDFbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Model 1 : Linear Regression"
      ],
      "metadata": {
        "id": "3OgdhrEU60pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define features (X) and target (y) assuming WQI is the target column\n",
        "X = Dataset.select_dtypes(include=np.number).drop('WQI', axis=1)\n",
        "y = Dataset['WQI']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R² Score:\", r2)\n"
      ],
      "metadata": {
        "id": "d9BAQCCV-4Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Model 2 : Decision Trees"
      ],
      "metadata": {
        "id": "IW5YrqWA68S1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 5: Create and train the Decision Tree Regressor\n",
        "dt_model = DecisionTreeRegressor(max_depth=5, random_state=42)  # max_depth is adjustable\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions and evaluate the model\n",
        "y_pred = dt_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Display the results\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"\\nSample Predictions:\")\n",
        "print(pd.DataFrame({'Actual WQI': y_test, 'Predicted WQI': y_pred}).head())"
      ],
      "metadata": {
        "id": "1NNWXkrv8f65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Model 3 : Random Forest Regression"
      ],
      "metadata": {
        "id": "6SS-BhT97Flw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 4: Create and train the Random Forest model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions and evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Display the results\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"\\nSample Predictions:\")\n",
        "print(pd.DataFrame({'Actual WQI': y_test, 'Predicted WQI': y_pred}).head())#"
      ],
      "metadata": {
        "id": "EalDXfTG8fUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Model 4 : Support Vector Resgression (SVR)"
      ],
      "metadata": {
        "id": "BYOc93h_7-El"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Step 5: Create and train the SVR model\n",
        "svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions and evaluate the model\n",
        "y_pred = svr_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Display the results\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"\\nSample Predictions:\")\n",
        "print(pd.DataFrame({'Actual WQI': y_test, 'Predicted WQI': y_pred}).head())"
      ],
      "metadata": {
        "id": "maRd_okN8hO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Model 5 : K-Nearest Neighbors Regression\n"
      ],
      "metadata": {
        "id": "R_bhjH898MgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "knn_model = KNeighborsRegressor(n_neighbors=5)  # You can adjust the number of neighbors\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions and evaluate the model\n",
        "y_pred = knn_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Display the results\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"\\nSample Predictions:\")\n",
        "print(pd.DataFrame({'Actual WQI': y_test, 'Predicted WQI': y_pred}).head())"
      ],
      "metadata": {
        "id": "u1Pu1k69KrOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Model 6 : Gradient Booster Regressor"
      ],
      "metadata": {
        "id": "sPDeqXd1Jqp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Step 5: Create and train the Gradient Boosting model\n",
        "gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbr_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions and evaluate the model\n",
        "y_pred = gbr_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Display the results\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "print(\"\\nSample Predictions:\")\n",
        "print(pd.DataFrame({'Actual WQI': y_test, 'Predicted WQI': y_pred}).head())"
      ],
      "metadata": {
        "id": "E6FdS5gF8h8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ML MODEL IMPLEMENTATION FOR HEALTH RISK PREDICTION"
      ],
      "metadata": {
        "id": "KPJBkXxCYSvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing feature engineering and creatinf a health risk"
      ],
      "metadata": {
        "id": "uqlNQ06uYSoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusted function with broader thresholds to potentially yield more favorable classifications\n",
        "def classify_health_risk_adjusted(row):\n",
        "    # pH classification with adjusted thresholds\n",
        "    if row['pH_Mean'] < 6.0 or row['pH_Mean'] > 9.0:\n",
        "        pH_risk = 'High Risk'\n",
        "    elif 7.0 <= row['pH_Mean'] <= 8.5:\n",
        "        pH_risk = 'Low Risk'\n",
        "    else:\n",
        "        pH_risk = 'Moderate Risk'\n",
        "\n",
        "    # TDS classification with adjusted thresholds\n",
        "    if row['TDS (mg/L)'] > 700:\n",
        "        TDS_risk = 'High Risk'\n",
        "    elif row['TDS (mg/L)'] <= 300:\n",
        "        TDS_risk = 'Low Risk'\n",
        "    else:\n",
        "        TDS_risk = 'Moderate Risk'\n",
        "\n",
        "    # Turbidity classification with adjusted thresholds\n",
        "    if row['Turbidity (NTU)'] > 10:\n",
        "        turbidity_risk = 'High Risk'\n",
        "    elif row['Turbidity (NTU)'] <= 3:\n",
        "        turbidity_risk = 'Low Risk'\n",
        "    else:\n",
        "        turbidity_risk = 'Moderate Risk'\n",
        "\n",
        "    # Combined risk level (simplified approach)\n",
        "    if 'High Risk' in [pH_risk, TDS_risk, turbidity_risk]:\n",
        "        return 'High Risk'\n",
        "    elif 'Moderate Risk' in [pH_risk, TDS_risk, turbidity_risk]:\n",
        "        return 'Moderate Risk'\n",
        "    else:\n",
        "        return 'Low Risk'\n",
        "\n",
        "# Apply the adjusted classification function to create a 'Health Risk' column.\n",
        "Dataset['Health Risk'] = Dataset.apply(classify_health_risk_adjusted, axis=1)\n",
        "\n",
        "# Show the distribution of health risk categories after adjustment\n",
        "Dataset['Health Risk'].value_counts()\n"
      ],
      "metadata": {
        "id": "BGRvtmm1YRKJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}